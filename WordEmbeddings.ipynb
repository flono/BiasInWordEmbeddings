{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from scipy import spatial\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You do not need to load both embeddings! For test runs, simply load the small embedding. It is much faster!\n",
    "embeddings_dict = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load big embedding into dictionary\n",
    "# embedding from http://vectors.nlpl.eu/repository/\n",
    "# English CoNLL17 corpus\n",
    "# This takes a while!\n",
    "\n",
    "embeddings_dict_big = {}\n",
    "\n",
    "counter = 0\n",
    "errors = []\n",
    "embeddings_dict = {}\n",
    "with open(\"/Users/floriannolte/Downloads/40/model.txt\", 'r', encoding='ISO-8859-1') as f: # encoding needed for special characters\n",
    "    for line in f:\n",
    "        counter += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        try:\n",
    "            vector = np.asarray(values[1:], \"float32\")\n",
    "            embeddings_dict_big[word] = vector\n",
    "        except:\n",
    "            errors.append((counter, word)) # save words where the program breaks for some reason\n",
    "\n",
    "# delete weird entries that have wrong dimensionality for some reason\n",
    "liste = []\n",
    "for k, v in embeddings_dict_big.items():\n",
    "    if np.shape(v)[0] != 100:\n",
    "        liste.append(k)\n",
    "\n",
    "for w in liste:\n",
    "    del embeddings_dict_big[w]\n",
    "    \n",
    "# del embeddings_dict[\"4027169\"] # first line is weird"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ALTERNATIVELY: load smol embedding into dictionary\n",
    "# this is the glove embedding from the authors of the paper\n",
    "# corpus: wikipedia 2014 + gigaword\n",
    "# https://nlp.stanford.edu/projects/glove/\n",
    "embeddings_dict_smol = {}\n",
    "\n",
    "with open(\"glove.6B.100d.txt\", 'r') as f: \n",
    "    for line in f:\n",
    "        counter += 1\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        vector = np.asarray(values[1:], \"float32\")\n",
    "        embeddings_dict_smol[word] = vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose embedding to work with\n",
    "\n",
    "embeddings_dict = embeddings_dict_smol\n",
    "#embeddings_dict = embeddings_dict_big"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2 x 50 random words to compare the companies against.\n",
    "random_words_1 = \"announcement friend ask pan tail quiet swarm calm glance level tolerate my part devote abortion memorial chaos bleed trench ensure mask create indulge salesperson conservative pier speculate genetic fiction regulation young harvest responsible beach squeeze seek field lip seasonal account complain angel dawn transport observation budge council sunshine farewell slide\".split(\" \")\n",
    "random_words_2 = \"healthy rage grain surprise trustee prison diagram promise shift escape amputate gallery note stress policeman edge foundation folk squeeze deck misery partnership distinct title hostage rate conspiracy sandwich abridge night old flourish crack angel example fibre urine courtship tragedy prove reform bush breakdown huge technology strip treat meal dignity wife\".split(\" \")\n",
    "random_words = random_words_1 + random_words_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function for finding closest words to one embedding\n",
    "# warning : very slow!\n",
    "\n",
    "def find_closest_embeddings(embedding):\n",
    "    return sorted(embeddings_dict.keys(), key=lambda word: spatial.distance.cosine(embeddings_dict[word], embedding))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns angle between two vectors / lists / arrays\n",
    "def angle_vec(vec1, vec2): \n",
    "    dot_product = np.dot(\n",
    "        np.array(vec1) / np.linalg.norm(np.array(vec1)),\n",
    "        np.array(vec2) / np.linalg.norm(np.array(vec2))\n",
    "    )\n",
    "    return np.degrees(np.arccos(dot_product))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Companies, frameworks and sentiment-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "companies = [\"Google\", \"Microsoft\", \"amazon\", \"Facebook\",\n",
    "             \"Intel\", \"Nvidia\", \"AMD\", \"Deepmind\", \"IBM\",\n",
    "             \"Boston\", \"Dynamics\", \"Softbank \", \"Robotics\",\n",
    "             \"PAL\", \" Robotics\", \"Willow\", \"Garage\",\n",
    "             \"Alibaba\", \" Cloud\", \"H20.ai\", \"Bosch\",\n",
    "             \"SPHERO\", \"ANYBOTS\", \"sarcos\", \"BARRETT\",\n",
    "             \"PETRONICS\", \"dronesense\", \"embodied\",\n",
    "             \"energid\" , \"irobot\", \"myomo\", \"vecna\"]\n",
    "\n",
    "companies = [name.lower().replace(\" \",\"\") for name in companies]\n",
    "\n",
    "\n",
    "frameworks = [\"Sci-kit\", \"Tensorflow\", \"Theano\", \"Caffe\", \"Keras\", \"Torch\", \"MicrosoftCNTK\",\n",
    "             \"OpenCV\", \"ROS\", \"OpenNN\", \"MxNet\", \"Amazon\", \"Matlab\", \"Accord.NET\",\n",
    "             \"Spark\",\"Mllib\", \"MLPack\", \"Apche\", \"Mahout\", \"Firebase\", \"Singa\",\n",
    "             \"Azure\", \"Google\", \"H20\", \"YARP\", \"MRPT\", \"Gazebo\",\n",
    "             \"OROCOS\", \"Spacy\", \"Facebook\", \"Gensim\"]\n",
    "\n",
    "frameworks = [name.lower().replace(\" \",\"\") for name in frameworks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google         True           \n",
      "microsoft      True           \n",
      "amazon         True           \n",
      "facebook       True           \n",
      "intel          True           \n",
      "nvidia         True           \n",
      "amd            True           \n",
      "deepmind       False          \n",
      "ibm            True           \n",
      "boston         True           \n",
      "dynamics       True           \n",
      "softbank       True           \n",
      "robotics       True           \n",
      "pal            True           \n",
      "robotics       True           \n",
      "willow         True           \n",
      "garage         True           \n",
      "alibaba        True           \n",
      "cloud          True           \n",
      "h20.ai         False          \n",
      "bosch          True           \n",
      "sphero         False          \n",
      "anybots        False          \n",
      "sarcos         True           \n",
      "barrett        True           \n",
      "petronics      False          \n",
      "dronesense     False          \n",
      "embodied       True           \n",
      "energid        False          \n",
      "irobot         True           \n",
      "myomo          False          \n",
      "vecna          True           \n"
     ]
    }
   ],
   "source": [
    "# view which companies are not in the embedding\n",
    "\n",
    "for i in companies:\n",
    "    try:\n",
    "        embeddings_dict[i]\n",
    "        print('%-15s%-15s' % (i, \"True\"))\n",
    "    except:\n",
    "        print('%-15s%-15s' % (i, \"False\"))\n",
    "    # look at \"printf-style String Formatting\" in https://docs.python.org/3/library/stdtypes.html for info about nice formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sci-kit        False          \n",
      "tensorflow     False          \n",
      "theano         True           \n",
      "caffe          True           \n",
      "keras          True           \n",
      "torch          True           \n",
      "microsoftcntk  False          \n",
      "opencv         False          \n",
      "ros            True           \n",
      "opennn         False          \n",
      "mxnet          False          \n",
      "amazon         True           \n",
      "matlab         True           \n",
      "accord.net     False          \n",
      "spark          True           \n",
      "mllib          False          \n",
      "mlpack         False          \n",
      "apche          False          \n",
      "mahout         True           \n",
      "firebase       True           \n",
      "singa          True           \n",
      "azure          True           \n",
      "google         True           \n",
      "h20            True           \n",
      "yarp           False          \n",
      "mrpt           False          \n",
      "gazebo         True           \n",
      "orocos         False          \n",
      "spacy          True           \n",
      "facebook       True           \n",
      "gensim         False          \n"
     ]
    }
   ],
   "source": [
    "# view which frameworks are not in the embedding\n",
    "\n",
    "for i in frameworks:\n",
    "    try:\n",
    "        embeddings_dict[i]\n",
    "        print('%-15s%-15s' % (i, \"True\"))\n",
    "    except:\n",
    "        print('%-15s%-15s' % (i, \"False\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simple positive and negative words\n",
    "\n",
    "positive_functional = \"secure, safe, robust, reliable, trustworthy, efficient, interactive, user-friendly, maintained, stable, outperform, convenient, intelligent, automatic, automated\".split(\", \")\n",
    "negative_functional = \"insecure, breach, slow, complicated, unstable\".split(\", \")\n",
    "positive_society = \"famous, open-source, fair, ethical\".split(\", \")\n",
    "negative_society = \"criticism, illegal, crime, fake, violate, unethical, biased, offensive\".split(\", \")\n",
    "\n",
    "# get embeddings for sentiment words\n",
    "p_f_embed = []\n",
    "for word in positive_functional:\n",
    "    p_f_embed.append(embeddings_dict[word])\n",
    "\n",
    "p_s_embed = []\n",
    "for word in positive_society:\n",
    "    p_s_embed.append(embeddings_dict[word])\n",
    "    \n",
    "n_f_embed = []\n",
    "for word in negative_functional:\n",
    "    n_f_embed.append(embeddings_dict[word])\n",
    "    \n",
    "n_s_embed = []\n",
    "for word in negative_society:\n",
    "    n_s_embed.append(embeddings_dict[word])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sentiment vectors from word pairs\n",
    "\n",
    "functional = [\n",
    "    (\"secure\",\"insecure\"),\n",
    "    (\"safe\",\"unsafe\"),\n",
    "    (\"reliable\",\"unreliable\"),\n",
    "    (\"complicated\",\"simple\"),\n",
    "    (\"fast\",\"slow\"),\n",
    "    (\"stable\",\"unstable\"),\n",
    "    (\"intelligent\",\"dumb\"),\n",
    "    (\"convenient\",\"inconvenient\")\n",
    "]\n",
    "society = [\n",
    "    (\"good\",\"bad\"),\n",
    "    (\"famous\",\"unknown\"),\n",
    "    (\"open-source\",\"locked\"),\n",
    "    (\"fair\",\"biased\"),\n",
    "    (\"legal\",\"illegal\"),\n",
    "    (\"ethical\",\"unethical\"),\n",
    "    (\"violate\",\"conform\"),\n",
    "    (\"criticism\",\"praise\")\n",
    "]\n",
    "\n",
    "# get embeddings for sentiment words\n",
    "functional_embed = []\n",
    "for pair in functional:\n",
    "    functional_embed.append(embeddings_dict[pair[1]] - embeddings_dict[pair[0]])\n",
    "\n",
    "society_embed = []\n",
    "for pair in society:\n",
    "    society_embed.append(embeddings_dict[pair[1]] - embeddings_dict[pair[0]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1st approach: Distances between companies and sentiment words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for every element, returns average distance to the words in the list (as list)\n",
    "def avg_dist(elements, words):\n",
    "    output = []\n",
    "    for elem in elements:\n",
    "        dist = 0\n",
    "        try:\n",
    "            for word in words:\n",
    "                dist += spatial.distance.cosine(embeddings_dict[elem], embeddings_dict[word])\n",
    "            dist = dist / len(words)\n",
    "        except: # elem not in vocabulary : do nothing\n",
    "            pass\n",
    "        output.append([elem, dist])\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "google     pos avg: 0.7634     neg avg: 0.8270    \n",
      "microsoft  pos avg: 0.7375     neg avg: 0.7713    \n",
      "amazon     pos avg: 0.8558     neg avg: 0.8590    \n",
      "facebook   pos avg: 0.8140     neg avg: 0.8442    \n",
      "intel      pos avg: 0.8235     neg avg: 0.8766    \n",
      "nvidia     pos avg: 0.9546     neg avg: 1.0255    \n",
      "amd        pos avg: 0.9065     neg avg: 0.9253    \n",
      "deepmind   pos avg: 0.0        neg avg: 0.0       \n",
      "ibm        pos avg: 0.7966     neg avg: 0.9035    \n",
      "boston     pos avg: 0.8360     neg avg: 0.8890    \n",
      "dynamics   pos avg: 0.8506     neg avg: 0.8327    \n",
      "softbank   pos avg: 0.9690     neg avg: 1.0173    \n",
      "robotics   pos avg: 0.8757     neg avg: 0.9630    \n",
      "pal        pos avg: 0.9701     neg avg: 0.9548    \n",
      "robotics   pos avg: 0.8757     neg avg: 0.9630    \n",
      "willow     pos avg: 0.9742     neg avg: 1.0101    \n",
      "garage     pos avg: 0.8365     neg avg: 0.9144    \n",
      "alibaba    pos avg: 1.0228     neg avg: 1.0450    \n",
      "cloud      pos avg: 0.8556     neg avg: 0.8225    \n",
      "h20.ai     pos avg: 0.0        neg avg: 0.0       \n",
      "bosch      pos avg: 0.9467     neg avg: 1.0351    \n",
      "sphero     pos avg: 0.0        neg avg: 0.0       \n",
      "anybots    pos avg: 0.0        neg avg: 0.0       \n",
      "sarcos     pos avg: 1.0425     neg avg: 1.0961    \n",
      "barrett    pos avg: 0.9554     neg avg: 0.9623    \n",
      "petronics  pos avg: 0.0        neg avg: 0.0       \n",
      "dronesense pos avg: 0.0        neg avg: 0.0       \n",
      "embodied   pos avg: 0.8049     neg avg: 0.8866    \n",
      "energid    pos avg: 0.0        neg avg: 0.0       \n",
      "irobot     pos avg: 1.0657     neg avg: 1.0822    \n",
      "myomo      pos avg: 0.0        neg avg: 0.0       \n",
      "vecna      pos avg: 1.1064     neg avg: 1.1189    \n"
     ]
    }
   ],
   "source": [
    "# print average distance to sentiment words (0.0 = not in dataset)\n",
    "\n",
    "p_f_comp = avg_dist(companies, positive_functional)\n",
    "n_f_comp = avg_dist(companies, negative_functional)\n",
    "p_s_comp = avg_dist(companies, positive_society)\n",
    "n_s_comp = avg_dist(companies, negative_society)\n",
    "\n",
    "for c in range(len(companies)):\n",
    "    print(\"%-10s pos avg: %-10.6s neg avg: %-10.6s\" % (p_f_comp[c][0], (p_f_comp[c][1] + p_s_comp[c][1])/2 , (n_f_comp[c][1] + n_s_comp[c][1])/2))\n",
    "    # % = value, - = left align, 10 = distance or position, .6 = precision, s = converting to string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2nd approach: Sentiment direction through PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# perform pca to get directions of greatest variance = sentiment vectors\n",
    "# PCA from simple word vectors (first PC of embeddings of functional / societal words)\n",
    "\n",
    "pca = PCA(n_components=1)\n",
    "principal_components = pca.fit(p_f_embed + n_f_embed)\n",
    "functional_sentiment_direction = principal_components.components_[0]\n",
    "\n",
    "principal_components = pca.fit(p_s_embed + n_s_embed)\n",
    "society_sentiment_direction = principal_components.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Functional  Societal\n",
      "google          103.5        90.72       \n",
      "microsoft       96.57        87.68       \n",
      "amazon          94.08        94.40       \n",
      "facebook        102.5        91.26       \n",
      "intel           91.03        87.24       \n",
      "nvidia          105.6        80.10       \n",
      "amd             93.58        80.45       \n",
      "deepmind\n",
      "ibm             97.80        86.24       \n",
      "boston          87.34        110.5       \n",
      "dynamics        92.45        83.45       \n",
      "softbank        88.38        85.39       \n",
      "robotics        106.3        73.58       \n",
      "pal             87.62        99.04       \n",
      "robotics        106.3        73.58       \n",
      "willow          87.56        86.49       \n",
      "garage          99.49        99.95       \n",
      "alibaba         97.14        81.96       \n",
      "cloud           89.89        96.56       \n",
      "h20.ai\n",
      "bosch           100.2        91.61       \n",
      "sphero\n",
      "anybots\n",
      "sarcos          98.33        75.15       \n",
      "barrett         87.09        92.83       \n",
      "petronics\n",
      "dronesense\n",
      "embodied        93.00        78.59       \n",
      "energid\n",
      "irobot          98.89        70.25       \n",
      "myomo\n",
      "vecna           97.52        86.18       \n",
      "sci-kit\n",
      "tensorflow\n",
      "theano          100.4        69.86       \n",
      "caffe           93.29        83.06       \n",
      "keras           91.82        82.02       \n",
      "torch           95.06        101.4       \n",
      "microsoftcntk\n",
      "opencv\n",
      "ros             77.55        86.83       \n",
      "opennn\n",
      "mxnet\n",
      "amazon          94.08        94.40       \n",
      "matlab          98.75        68.25       \n",
      "accord.net\n",
      "spark           91.58        102.1       \n",
      "mllib\n",
      "mlpack\n",
      "apche\n",
      "mahout          93.84        83.11       \n",
      "firebase        98.10        82.03       \n",
      "singa           95.85        75.01       \n",
      "azure           90.76        80.75       \n",
      "google          103.5        90.72       \n",
      "h20             88.93        78.51       \n",
      "yarp\n",
      "mrpt\n",
      "gazebo          96.37        83.92       \n",
      "orocos\n",
      "spacy           94.10        75.53       \n",
      "facebook        102.5        91.26       \n",
      "gensim\n"
     ]
    }
   ],
   "source": [
    "# print and save all words with their angles to society words and functional words\n",
    "# angles around 90° are good.\n",
    "\n",
    "angles_soc_func = []\n",
    "\n",
    "print(\"%-15s%-10s%10s\" % (\"\", \"Functional\", \"Societal\"))\n",
    "for word in companies + frameworks:\n",
    "    try:\n",
    "        angles_soc_func.append((word, angle_vec(embeddings_dict[word], functional_sentiment_direction), angle_vec(embeddings_dict[word], society_sentiment_direction)))\n",
    "        print(\"%-15s %-12.5s %-12.5s\" % (angles_soc_func[-1]))\n",
    "    except:\n",
    "        angles_soc_func.append((word, None, None))\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save values for reference, Dirty Hack\n",
    "values = []\n",
    "for word in companies + frameworks:\n",
    "    try:\n",
    "        values.append((word, angle_vec(embeddings_dict[word], functional_sentiment_direction), angle_vec(embeddings_dict[word], society_sentiment_direction)))\n",
    "    except:\n",
    "        values.append((word, None, None))\n",
    "\n",
    "for i in values:\n",
    "    pass#print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# output of the above code, using the big embedding\n",
    "data_40 = [('google', 81.58132197111395, 85.38306064697701) ,\n",
    "('microsoft', 78.36052999867374, 77.0228811916095) ,\n",
    "('amazon', 85.28186932924604, 85.933387302177) ,\n",
    "('facebook', 88.9508847830484, 94.82193448350525) ,\n",
    "('intel', 94.08120556760542, 78.0237077154805) ,\n",
    "('nvidia', 90.33171314960701, 78.04591544990853) ,\n",
    "('amd', 95.76111446345796, 79.08873679959837) ,\n",
    "('deepmind', 78.92806778865676, 77.36679111129862) ,\n",
    "('ibm', 85.45635063356441, 72.9587140422921) ,\n",
    "('boston', 83.06760921028163, 94.60454058870201) ,\n",
    "('dynamics', 80.47870836776725, 86.08774306505475) ,\n",
    "('softbank', 84.02908052690933, 86.31367386642376) ,\n",
    "('robotics', 72.8452740120154, 77.73747258185175) ,\n",
    "('pal', 92.52405433992357, 87.40972688031428) ,\n",
    "('robotics', 72.8452740120154, 77.73747258185175) ,\n",
    "('willow', 92.7878038847685, 86.38526571500144) ,\n",
    "('garage', 83.87941769241495, 87.08019065492876) ,\n",
    "('alibaba', 89.0550431573287, 82.9957963645851) ,\n",
    "('cloud', 83.55522729406717, 74.80600440144579) ,\n",
    "('h20.ai', None, None) ,\n",
    "('bosch', 76.4877570463206, 81.03077193341183) ,\n",
    "('sphero', 72.51901639439613, 84.38930002249519) ,\n",
    "('anybots', 75.6938960914384, 70.93412811473013) ,\n",
    "('sarcos', 82.00368535641135, 75.24767581781794) ,\n",
    "('barrett', 91.09864319928245, 98.02343856986285) ,\n",
    "('petronics', None, None) ,\n",
    "('dronesense', None, None) ,\n",
    "('embodied', 89.79062034052514, 91.91506877285539) ,\n",
    "('energid', 81.39549770473579, 82.52686880880094) ,\n",
    "('irobot', 77.07486632081178, 86.59895044418644) ,\n",
    "('myomo', 79.51423967820855, 82.5475819227478) ,\n",
    "('vecna', 88.31910159971396, 84.05769716800604) ,\n",
    "('sci-kit', None, None) ,\n",
    "('tensorflow', 74.35898851036781, 65.92541033632993) ,\n",
    "('theano', 90.68707080148246, 68.46808992119101) ,\n",
    "('caffe', 81.23021449824734, 75.13977778039687) ,\n",
    "('keras', 79.23473723063465, 77.70228314026521) ,\n",
    "('torch', 78.74085813471446, 88.22241981047789) ,\n",
    "('microsoftcntk', None, None) ,\n",
    "('opencv', 77.94834113062669, 68.70011051763392) ,\n",
    "('ros', 98.11706339713135, 84.5791374540656) ,\n",
    "('opennn', 78.47516401181447, 74.83088873563786) ,\n",
    "('mxnet', None, None) ,\n",
    "('amazon', 85.28186932924604, 85.933387302177) ,\n",
    "('matlab', 77.76507167583912, 70.68675064204588) ,\n",
    "('accord.net', 76.014251989394, 73.09254826313085) ,\n",
    "('spark', 82.49301733833002, 93.15978921747266) ,\n",
    "('mllib', 74.23613598685112, 71.6953677491158) ,\n",
    "('mlpack', 82.0229633843731, 75.50560541659044) ,\n",
    "('apche', 85.77292370539041, 79.03605900575565) ,\n",
    "('mahout', 93.71051699125363, 76.08611873701652) ,\n",
    "('firebase', 86.77727029960036, 89.59668497799792) ,\n",
    "('singa', 86.71639641299284, 79.83877368949364) ,\n",
    "('azure', 78.23977991962062, 76.18484223521166) ,\n",
    "('google', 81.58132197111395, 85.38306064697701) ,\n",
    "('h20', 86.29645020702574, 81.54305714291549) ,\n",
    "('yarp', 81.74074656605538, 79.96225260628653) ,\n",
    "('mrpt', 74.81719981856898, 75.36126701799999) ,\n",
    "('gazebo', 87.82167923735082, 80.21430713443701) ,\n",
    "('orocos', 82.30713731088018, 69.75631515387113) ,\n",
    "('spacy', 86.51560971466118, 87.6341989870835) ,\n",
    "('facebook', 88.9508847830484, 94.82193448350525) ,\n",
    "('gensim', 81.0125357525128, 72.65438778336181)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# PCA from vectors of word pairs\n",
    "# first PC of vectors of word pairs\n",
    "\n",
    "pca2 = PCA(n_components=1)\n",
    "\n",
    "principal_components2 = pca2.fit(functional_embed)\n",
    "functional_sentiment_direction2 = principal_components2.components_[0]\n",
    "\n",
    "principal_components2 = pca2.fit(society_embed)\n",
    "society_sentiment_direction2 = principal_components2.components_[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               Functional  Societal\n",
      "google          82.82        92.48       \n",
      "microsoft       82.27        89.67       \n",
      "amazon          89.52        95.36       \n",
      "facebook        85.79        93.52       \n",
      "intel           83.55        91.79       \n",
      "nvidia          94.19        107.4       \n",
      "amd             91.14        100.0       \n",
      "deepmind\n",
      "ibm             80.27        90.40       \n",
      "boston          73.47        73.80       \n",
      "dynamics        97.16        86.67       \n",
      "softbank        93.96        96.05       \n",
      "robotics        97.30        103.0       \n",
      "pal             89.35        84.90       \n",
      "robotics        97.30        103.0       \n",
      "willow          86.68        92.71       \n",
      "garage          72.96        78.45       \n",
      "alibaba         96.30        95.98       \n",
      "cloud           88.42        81.47       \n",
      "h20.ai\n",
      "bosch           88.70        86.77       \n",
      "sphero\n",
      "anybots\n",
      "sarcos          94.44        94.74       \n",
      "barrett         86.96        90.70       \n",
      "petronics\n",
      "dronesense\n",
      "embodied        92.55        92.70       \n",
      "energid\n",
      "irobot          103.7        97.00       \n",
      "myomo\n",
      "vecna           99.74        96.54       \n",
      "sci-kit\n",
      "tensorflow\n",
      "theano          98.70        101.3       \n",
      "caffe           91.13        102.0       \n",
      "keras           92.33        96.05       \n",
      "torch           84.66        82.29       \n",
      "microsoftcntk\n",
      "opencv\n",
      "ros             93.28        86.88       \n",
      "opennn\n",
      "mxnet\n",
      "amazon          89.52        95.36       \n",
      "matlab          84.42        116.1       \n",
      "accord.net\n",
      "spark           82.62        83.14       \n",
      "mllib\n",
      "mlpack\n",
      "apche\n",
      "mahout          92.38        102.9       \n",
      "firebase        93.56        98.50       \n",
      "singa           93.31        102.3       \n",
      "azure           77.06        90.15       \n",
      "google          82.82        92.48       \n",
      "h20             102.1        99.47       \n",
      "yarp\n",
      "mrpt\n",
      "gazebo          84.01        90.32       \n",
      "orocos\n",
      "spacy           102.5        104.0       \n",
      "facebook        85.79        93.52       \n",
      "gensim\n"
     ]
    }
   ],
   "source": [
    "# print and save all words with their angles to society words and functional words\n",
    "# angles around 90° are good.\n",
    "\n",
    "angles_soc_func2 = []\n",
    "\n",
    "print(\"%-15s%-10s%10s\" % (\"\", \"Functional\", \"Societal\"))\n",
    "for word in companies + frameworks:\n",
    "    try:\n",
    "        angles_soc_func2.append((word, angle_vec(embeddings_dict[word], functional_sentiment_direction2), angle_vec(embeddings_dict[word], society_sentiment_direction2)))\n",
    "        print(\"%-15s %-12.5s %-12.5s\" % angles_soc_func2[-1])\n",
    "    except:\n",
    "        angles_soc_func2.append((word, None, None))\n",
    "        print(word)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print and save all random words with their angles to society words and functional words\n",
    "# angles around 90° are good.\n",
    "\n",
    "angles_soc_func_random = []\n",
    "\n",
    "#print(\"%-15s%-10s%10s\" % (\"\", \"Functional\", \"Societal\"))\n",
    "for word in random_words:\n",
    "    angles_soc_func_random.append((word, angle_vec(embeddings_dict[word], functional_sentiment_direction2), angle_vec(embeddings_dict[word], society_sentiment_direction2)))\n",
    "    #print(\"%-15s %-12.5s %-12.5s\" % angles_soc_func_random[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code to save values for reference (dirty hack to avoid saving and loading files)\n",
    "values2 = []\n",
    "for word in companies + frameworks:\n",
    "    try:\n",
    "        values2.append((word, angle_vec(embeddings_dict[word], functional_sentiment_direction2), angle_vec(embeddings_dict[word], society_sentiment_direction2)))\n",
    "    except:\n",
    "        values2.append((word, None, None))\n",
    "\n",
    "for i in values2:\n",
    "    pass#print(i, \",\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_40_2 = [('google', 87.80729601569945, 101.2616002084186) ,\n",
    "('microsoft', 92.22947565432817, 110.07164865347103) ,\n",
    "('amazon', 84.22298519162408, 98.37073957342788) ,\n",
    "('facebook', 91.19328028084082, 91.51849020253316) ,\n",
    "('intel', 94.42998450709902, 103.26136216765518) ,\n",
    "('nvidia', 93.46834033729112, 103.31779371734459) ,\n",
    "('amd', 96.12119166958179, 102.65396343584038) ,\n",
    "('deepmind', 93.21084544177033, 106.0669040520475) ,\n",
    "('ibm', 94.03396988383408, 107.96697749573657) ,\n",
    "('boston', 85.78426620602532, 84.25549514851684) ,\n",
    "('dynamics', 101.48977662302497, 99.79254005180135) ,\n",
    "('softbank', 85.47946255530569, 96.38697521969269) ,\n",
    "('robotics', 91.68728435086771, 105.52983858549132) ,\n",
    "('pal', 89.9367749521248, 92.88737420651849) ,\n",
    "('robotics', 91.68728435086771, 105.52983858549132) ,\n",
    "('willow', 88.20087430511555, 88.78103146006853) ,\n",
    "('garage', 84.01158632758941, 78.51120196992503) ,\n",
    "('alibaba', 86.58470898717835, 91.20183815662308) ,\n",
    "('cloud', 94.23950569972763, 102.21021603083739) ,\n",
    "('h20.ai', None, None) ,\n",
    "('bosch', 86.13300389051822, 91.6526205537132) ,\n",
    "('sphero', 78.37479529498589, 94.67425651443624) ,\n",
    "('anybots', 93.58907725450175, 107.57486805809884) ,\n",
    "('sarcos', 93.12075022620122, 102.3142291626249) ,\n",
    "('barrett', 90.83283905628821, 82.86276948744549) ,\n",
    "('petronics', None, None) ,\n",
    "('dronesense', None, None) ,\n",
    "('embodied', 105.61961368407982, 95.21941810702579) ,\n",
    "('energid', 92.05817894014919, 101.3757289172342) ,\n",
    "('irobot', 90.21114339159243, 93.85870258248171) ,\n",
    "('myomo', 86.759551734634, 100.41563664270775) ,\n",
    "('vecna', 94.47565428101133, 93.75927845084506) ,\n",
    "('sci-kit', None, None) ,\n",
    "('tensorflow', 91.32465113521452, 116.59238909086451) ,\n",
    "('theano', 94.5420832646147, 106.78243963831068) ,\n",
    "('caffe', 80.3433866463473, 95.21042232629418) ,\n",
    "('keras', 85.64530001808315, 98.65104151898463) ,\n",
    "('torch', 78.73021722446897, 88.20401403330786) ,\n",
    "('microsoftcntk', None, None) ,\n",
    "('opencv', 86.4819957335614, 111.75579056042733) ,\n",
    "('ros', 98.6049338762749, 98.51974202542617) ,\n",
    "('opennn', 93.20902951597955, 107.82866292336239) ,\n",
    "('mxnet', None, None) ,\n",
    "('amazon', 84.22298519162408, 98.37073957342788) ,\n",
    "('matlab', 89.77010404797144, 115.21627303053977) ,\n",
    "('accord.net', 87.9976172592865, 111.42730847075244) ,\n",
    "('spark', 92.82180187353768, 89.45442769250474) ,\n",
    "('mllib', 88.78843620486964, 109.96137941920102) ,\n",
    "('mlpack', 88.26684272522562, 107.59817568289475) ,\n",
    "('apche', 88.87912163757164, 101.82474544321359) ,\n",
    "('mahout', 98.75995334973769, 104.13602937843352) ,\n",
    "('firebase', 90.76320736267996, 95.3263933014051) ,\n",
    "('singa', 91.57197742189429, 101.73901998597614) ,\n",
    "('azure', 88.16905290339425, 97.54895918444824) ,\n",
    "('google', 87.80729601569945, 101.2616002084186) ,\n",
    "('h20', 87.21967183950501, 96.55250398915561) ,\n",
    "('yarp', 89.09333457641748, 105.49539115557148) ,\n",
    "('mrpt', 89.2149421989122, 110.31942666080192) ,\n",
    "('gazebo', 78.12644521584215, 83.15024196561349) ,\n",
    "('orocos', 93.09475342441513, 109.4087414431444) ,\n",
    "('spacy', 91.52630880672592, 89.23446259845957) ,\n",
    "('facebook', 91.19328028084082, 91.51849020253316) ,\n",
    "('gensim', 96.06760896091329, 112.06899783278402)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87.32351279397706\n",
      "90.72444551618975\n"
     ]
    }
   ],
   "source": [
    "print(angle_vec(embeddings_dict[\"war\"], society_sentiment_direction2))\n",
    "print(angle_vec(embeddings_dict[\"bad\"], society_sentiment_direction2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "96.1801"
      ]
     },
     "execution_count": 170,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angle_vec(embeddings_dict[\"king\"] - embeddings_dict[\"queen\"], embeddings_dict[\"secretary\"] - embeddings_dict[\"coach\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-159-4a289d4f13c9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfind_closest_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"dachshund\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e1577bfcd8aa>\u001b[0m in \u001b[0;36mfind_closest_embeddings\u001b[0;34m(embedding)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_closest_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-3-e1577bfcd8aa>\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(word)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mfind_closest_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0msorted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeys\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mlambda\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mspatial\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistance\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcosine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0membeddings_dict\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mword\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0membedding\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "find_closest_embeddings(embeddings_dict[\"dachshund\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "functional bias:  9.304432622842576 societal bias:  10.992649064172603\n"
     ]
    }
   ],
   "source": [
    "# average absolute value of the distance between 90 degrees and the angle of the company vectors to the \"society\" direction\n",
    "# measure of how biased the embedding is (between 0 and 90)\n",
    "\n",
    "avg_societal_bias = 0\n",
    "avg_functional_bias = 0\n",
    "counter = 0\n",
    "\n",
    "for line in angles_soc_func_random:\n",
    "    angle_soc = line[2]\n",
    "    angle_func = line[1]\n",
    "    \n",
    "    if angle_soc: # check if there is data for the company / framework\n",
    "        counter += 1\n",
    "        avg_societal_bias += abs(90 - angle_soc)\n",
    "        avg_functional_bias += abs(90 - angle_func)\n",
    "    \n",
    "avg_functional_bias /= counter\n",
    "avg_societal_bias /= counter\n",
    "\n",
    "print(\"functional bias: \", avg_functional_bias, \"societal bias: \" , avg_societal_bias)\n",
    "# CoNLL17: functional bias:  3.9495742106369267 societal bias:  11.044453649519795\n",
    "# GloVe: functional bias:  5.988432694507943 societal bias:  7.173770563704972\n",
    "# Random: functional bias:  9.304432622842576 societal bias:  10.992649064172603"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
